import os
import csv
import json
import requests
import psycopg2
from datetime import datetime

# --- 1. ì™¸ë¶€ DB ì—°ê²° ì„¤ì • (DBeaver ì •ë³´ ë°˜ì˜) ---
DB_HOST = "pg-3ae9p5.vpc-cdb-kr.ntruss.com"
DB_PORT = "5432"
DB_NAME = "qmarket"
DB_USER = "hansol"
DB_PASSWORD = os.environ.get("DB_PASSWORD")  # GitHub Secretsì—ì„œ ê°€ì ¸ì˜´

# --- 2. Supabase ì„¤ì • ---
# URLì€ ê³µê°œë˜ì–´ë„ í° ë¬¸ì œ ì—†ìœ¼ë‚˜, KeyëŠ” ì ˆëŒ€ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤.
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")  # Service Role Key í•„ìˆ˜
SUPABASE_TABLE = "orders"  # ì—…ë¡œë“œí•  í…Œì´ë¸” ì´ë¦„ (í•„ìš”ì‹œ ìˆ˜ì •)

# --- 3. ì¶”ì¶œí•  ì¿¼ë¦¬ (ìˆ˜ì • í•„ìš”) ---
# ì˜ˆ: ì–´ì œ í•˜ë£¨ ë™ì•ˆ ìƒì„±ëœ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ê¸°
SQL_QUERY = "SELECT * FROM orders WHERE created_at >= NOW() - INTERVAL '1 day';"
CSV_FILE_PATH = "exported_data.csv"
BATCH_SIZE = 1000

def extract_db_to_csv():
    print("ğŸ”„ [1ë‹¨ê³„] ì™¸ë¶€ DBì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")
    try:
        conn = psycopg2.connect(
            host=DB_HOST, database=DB_NAME, user=DB_USER, 
            password=DB_PASSWORD, port=DB_PORT
        )
        cursor = conn.cursor()
        cursor.execute(SQL_QUERY)
        
        # CSV íŒŒì¼ ì‘ì„±
        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # ì»¬ëŸ¼ëª…(Header) ì‘ì„±
            if cursor.description:
                headers = [desc[0] for desc in cursor.description]
                writer.writerow(headers)
            # ë°ì´í„° ì‘ì„±
            writer.writerows(cursor)
            
        cursor.close()
        conn.close()
        print(f"âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ ({CSV_FILE_PATH})")
        return True
    except Exception as e:
        print(f"âŒ DB ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return False

def upload_csv_to_supabase():
    print("ğŸ”„ [2ë‹¨ê³„] Supabaseë¡œ ë°ì´í„° ì—…ë¡œë“œ ì‹œì‘...")
    headers = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "return=minimal" # ì‘ë‹µ ìµœì†Œí™” (ì†ë„ í–¥ìƒ)
    }

    try:
        with open(CSV_FILE_PATH, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f) # í—¤ë”ë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ë³€í™˜
            data_batch = []
            count = 0
            
            for row in reader:
                data_batch.append(row)
                if len(data_batch) >= BATCH_SIZE:
                    _send_batch(data_batch, headers)
                    count += len(data_batch)
                    data_batch = [] # ì´ˆê¸°í™”
            
            # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
            if data_batch:
                _send_batch(data_batch, headers)
                count += len(data_batch)
                
            print(f"âœ… ì´ {count}ê°œ ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ.")
            
    except FileNotFoundError:
        print("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def _send_batch(data, headers):
    url = f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE}"
    response = requests.post(url, headers=headers, data=json.dumps(data))
    if response.status_code != 201:
        print(f"âš ï¸ ì—…ë¡œë“œ ê²½ê³  (Code {response.status_code}): {response.text}")

if __name__ == "__main__":
    if extract_db_to_csv():
        upload_csv_to_supabase()
