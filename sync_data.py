import os
import csv
import json
import requests
import psycopg2
from datetime import datetime
from io import StringIO

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ---
DB_PASSWORD = os.environ.get("DB_PASSWORD")
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# ì™¸ë¶€ PostgreSQL DB ì ‘ì† ì •ë³´
DB_HOST = "pg-3ae9p5.vpc-cdb-kr.ntruss.com"
DB_PORT = "5432"
DB_NAME = "qmarket"
DB_USER = "hansol"

# --- ë°°ì¹˜ ë° ì„ì‹œ íŒŒì¼ ì„¤ì • ---
BATCH_SIZE = 1000
TEMP_DIR = "/tmp" 

# --- SQL ì¿¼ë¦¬ ì •ì˜ ë° Supabase í…Œì´ë¸” ì´ë¦„ ë§¤í•‘ ---
QUERIES = {
    "users": {
        "sql": """
            WITH latest_user_addresses AS (
                SELECT DISTINCT ON (user_id) user_id, mart_id
                FROM user_addresses
                ORDER BY user_id, user_address_id DESC
            ),
            marketing_pivot AS (
                SELECT
                    user_id,
                    BOOL_OR(CASE WHEN marketing_type = 'APP_PUSH' THEN agreement ELSE NULL END) AS agree_app_push,
                    BOOL_OR(CASE WHEN marketing_type = 'MESSAGE' THEN agreement ELSE NULL END) AS agree_sms,
                    BOOL_OR(CASE WHEN marketing_type = 'ALIM_TALK' THEN agreement ELSE NULL END) AS agree_alim_talk
                FROM marketing_agreements
                GROUP BY user_id
            )
            SELECT
                u.user_id, u.nickname, u.phone, u.created_date AS signup_date, g.name AS grade_name,
                m.mart_id, m.name AS mart_name, m.enabled AS mart_is_enabled,
                mp.agree_app_push, mp.agree_sms, mp.agree_alim_talk
            FROM users u
            LEFT JOIN grades g ON u.grade = g.code
            LEFT JOIN latest_user_addresses lua ON u.user_id = lua.user_id
            LEFT JOIN marts m ON lua.mart_id = m.mart_id
            LEFT JOIN marketing_pivot mp ON u.user_id = mp.user_id
            WHERE u.user_status = 'ACTIVE'
            AND (m.enabled = true AND m.closed != 'ì—°ì¤‘íœ´ë¬´');
        """,
        "on_conflict": "user_id",
        "delete_orphans": True,  # ì†ŒìŠ¤ì— ì—†ëŠ” ë°ì´í„° ì‚­ì œ
        "key_columns": ["user_id"]  # ì‚­ì œ ì‹œ ë¹„êµí•  í‚¤
    },
    
    "coupons": {
        "sql": """
            SELECT
                u.user_id,
                COUNT(uc.user_coupon_id) AS valid_coupon_count,
                ARRAY_AGG(uc.coupon_id) AS valid_coupon_ids
            FROM users u
            JOIN user_coupons uc ON u.user_id = uc.user_id
            WHERE u.user_status = 'ACTIVE'
              AND uc.is_used = false
              AND uc.end_date > CURRENT_DATE
            GROUP BY u.user_id
            ORDER BY valid_coupon_count DESC;
        """,
        "on_conflict": "user_id",
        "delete_orphans": True,
        "key_columns": ["user_id"]
    },
    
    "orders": {
        "sql": """
            SELECT
                o.user_id, o.order_id, o.created_date AS order_date,
                SUM(CASE WHEN p.type NOT IN ('QMONEY', 'COUPON', 'DELIVERY_COUPON') THEN p.payment ELSE 0 END) AS real_payment,
                SUM(CASE WHEN p.type = 'QMONEY' THEN p.payment ELSE 0 END) AS qmoney_payment,
                SUM(CASE WHEN p.type IN ('COUPON', 'DELIVERY_COUPON') THEN p.payment ELSE 0 END) AS coupon_payment
            FROM orders o
            JOIN payments p ON o.order_id = p.order_id
            WHERE o.state = 'PURCHASE_COMPLETED'
              AND p.state = 'COMPLETE' 
              AND o.created_date >= (CURRENT_DATE - INTERVAL '1 year')
            GROUP BY o.user_id, o.order_id, o.created_date
            ORDER BY o.user_id, o.created_date;
        """,
        "on_conflict": "user_id,order_id",  # ë³µí•©í‚¤ë¡œ ìˆ˜ì •
        "delete_orphans": True,
        "key_columns": ["user_id", "order_id"]
    }
}

def extract_db_to_csv(query_config, temp_filename):
    """ì™¸ë¶€ DBì—ì„œ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³  CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not DB_PASSWORD:
        raise ValueError("DB_PASSWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    print(f"\n--- [1ë‹¨ê³„] {temp_filename} ì¶”ì¶œ ì‹œì‘ ---")
    try:
        conn = psycopg2.connect(
            host=DB_HOST, database=DB_NAME, user=DB_USER, 
            password=DB_PASSWORD, port=DB_PORT
        )
        cursor = conn.cursor()
        cursor.execute(query_config['sql'])
        
        filepath = os.path.join(TEMP_DIR, temp_filename)
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if cursor.description:
                headers = [desc[0] for desc in cursor.description]
                writer.writerow(headers)
            writer.writerows(cursor)
            
        cursor.close()
        conn.close()
        print(f"âœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {filepath}")
        return filepath
    except Exception as e:
        print(f"âŒ DB ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def _send_batch(data, table_name, on_conflict_col):
    """Supabase APIë¡œ ë°°ì¹˜ ë°ì´í„°ë¥¼ Upsert ì „ì†¡"""
    headers = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json",
        "Prefer": "resolution=merge-duplicates"  # upsert ë™ì‘ ëª…ì‹œ
    }
    
    params = {}
    if on_conflict_col:
        params['on_conflict'] = on_conflict_col
    
    url = f"{SUPABASE_URL}/rest/v1/{table_name}"
    response = requests.post(url, headers=headers, params=params, data=json.dumps(data))
    
    if 200 <= response.status_code < 300:
        return True
    else:
        print(f"âš ï¸ {table_name} Upsert ì‹¤íŒ¨ (Code {response.status_code}): {response.text}")
        return False

def delete_orphaned_records(table_name, key_columns, current_keys):
    """Supabaseì—ì„œ ì†ŒìŠ¤ DBì— ì—†ëŠ” ë ˆì½”ë“œ ì‚­ì œ"""
    print(f"\n--- [3ë‹¨ê³„] {table_name}ì—ì„œ orphan ë ˆì½”ë“œ ì‚­ì œ ì‹œì‘ ---")
    
    headers = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }
    
    try:
        # 1. Supabaseì—ì„œ í˜„ì¬ ëª¨ë“  í‚¤ ì¡°íšŒ
        url = f"{SUPABASE_URL}/rest/v1/{table_name}"
        select_cols = ",".join(key_columns)
        params = {"select": select_cols}
        
        response = requests.get(url, headers=headers, params=params)
        if response.status_code != 200:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {response.text}")
            return
        
        existing_records = response.json()
        
        # 2. ì‚­ì œí•  í‚¤ ì°¾ê¸°
        existing_keys = set()
        for record in existing_records:
            key_tuple = tuple(str(record[col]) for col in key_columns)
            existing_keys.add(key_tuple)
        
        keys_to_delete = existing_keys - current_keys
        
        if not keys_to_delete:
            print(f"âœ… ì‚­ì œí•  orphan ë ˆì½”ë“œ ì—†ìŒ")
            return
        
        print(f"ğŸ—‘ï¸  {len(keys_to_delete)}ê°œì˜ orphan ë ˆì½”ë“œ ë°œê²¬, ì‚­ì œ ì¤‘...")
        
        # 3. ì‚­ì œ ì‹¤í–‰
        deleted_count = 0
        for key_tuple in keys_to_delete:
            # ë³µí•©í‚¤ ì¡°ê±´ ìƒì„±
            conditions = []
            for i, col in enumerate(key_columns):
                conditions.append(f"{col}=eq.{key_tuple[i]}")
            
            delete_url = f"{url}?{'&'.join(conditions)}"
            del_response = requests.delete(delete_url, headers=headers)
            
            if 200 <= del_response.status_code < 300:
                deleted_count += 1
            else:
                print(f"âš ï¸ ì‚­ì œ ì‹¤íŒ¨ {key_tuple}: {del_response.text}")
        
        print(f"âœ… {deleted_count}ê°œ ë ˆì½”ë“œ ì‚­ì œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ Orphan ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")

def upload_csv_to_supabase(table_name, filepath, config):
    """CSV íŒŒì¼ì„ ì½ì–´ Supabase REST APIë¥¼ í†µí•´ ë°°ì¹˜ Upsert ë° orphan ì‚­ì œ"""
    on_conflict_col = config.get("on_conflict")
    print(f"\n--- [2ë‹¨ê³„] {table_name} ì—…ë¡œë“œ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}) ---")
    
    if not os.path.exists(filepath):
        print(f"âŒ íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜: {filepath}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    current_keys = set()  # í˜„ì¬ ì†ŒìŠ¤ DBì— ìˆëŠ” í‚¤ë“¤
    
    try:
        with open(filepath, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            data_batch = []
            total_count = 0
            
            for row in reader:
                data_batch.append(row)
                
                # í‚¤ ì¶”ì 
                if config.get("delete_orphans") and config.get("key_columns"):
                    key_tuple = tuple(str(row[col]) for col in config["key_columns"])
                    current_keys.add(key_tuple)
                
                if len(data_batch) >= BATCH_SIZE:
                    if _send_batch(data_batch, table_name, on_conflict_col):
                        total_count += len(data_batch)
                        print(f"âœ… {len(data_batch)}ê°œ ë°°ì¹˜ ì„±ê³µ. ëˆ„ì : {total_count}")
                        data_batch = []
                    else:
                        print(f"âŒ {table_name} ì—…ë¡œë“œ ì¤‘ë‹¨ë¨.")
                        return

            # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
            if data_batch:
                if _send_batch(data_batch, table_name, on_conflict_col):
                    total_count += len(data_batch)
                    print(f"âœ… ìµœì¢… ë‚¨ì€ {len(data_batch)}ê°œ ë°°ì¹˜ ì„±ê³µ. ì´ {total_count}ê°œ ì²˜ë¦¬.")
                else:
                    print(f"âŒ {table_name} ìµœì¢… ì—…ë¡œë“œ ì¤‘ë‹¨ë¨.")
                    return
        
        # Orphan ë ˆì½”ë“œ ì‚­ì œ
        if config.get("delete_orphans") and config.get("key_columns"):
            delete_orphaned_records(table_name, config["key_columns"], current_keys)

    except Exception as e:
        print(f"âŒ {table_name} ì—…ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
    finally:
        if os.path.exists(filepath):
            os.remove(filepath)
            print(f"âœ… ì„ì‹œ íŒŒì¼ ì‚­ì œ: {filepath}")

def run_all_syncs():
    """ì •ì˜ëœ ëª¨ë“  ì¿¼ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰"""
    for table_name, config in QUERIES.items():
        print(f"\n==================== {table_name.upper()} ë™ê¸°í™” ì‹œì‘ ====================")
        temp_filename = f"{table_name}_export.csv"
        
        # 1. ì¶”ì¶œ
        filepath = extract_db_to_csv(config, temp_filename)
        
        # 2. ì—…ë¡œë“œ ë° ì‚­ì œ
        if filepath:
            if not config.get("on_conflict"):
                print(f"âš ï¸ {table_name} í…Œì´ë¸”ì˜ 'on_conflict' í‚¤ê°€ ì •ì˜ë˜ì§€ ì•Šì•„ **Insertë§Œ** ì‹œë„í•©ë‹ˆë‹¤.")
            
            upload_csv_to_supabase(table_name, filepath, config)

if __name__ == "__main__":
    if not (SUPABASE_URL and SUPABASE_KEY and DB_PASSWORD):
        print("âŒ í™˜ê²½ ë³€ìˆ˜ (SUPABASE_URL, SUPABASE_KEY, DB_PASSWORD)ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        run_all_syncs()
